Nice — I created a working proof-of-concept single-file CLI tool and saved it to:

/mnt/data/audio_chopper.py — [Download it from your session files]

What I built for you (POC)
	•	CLI script audio_chopper.py that:
	•	Optionally identifies the track using AcoustID (needs fpcalc + AcoustID API key).  ￼
	•	Optionally separates stems using Demucs (calls demucs CLI).  ￼ ￼
	•	Transcribes audio locally using OpenAI Whisper if you have it installed (no paid API required). Whisper gives segment timestamps which the script uses to seed vocal chops.  ￼ ￼
	•	Uses librosa for onset detection and RMS/dynamics analysis to create percussive/drone chops and to inform chop boundaries.  ￼
	•	Saves everything into the folder structure you requested:

[artist - song]/source/Artist - SongName.wav
                  /stems/    (demucs output, if available)
                  /transcript/transcript.txt
                  /chops/    (files named like 14.000-3.200-vocal-wake_up.wav)


	•	Adds logging and CLI verbosity flags and shows progress on longer loops with tqdm.

Important notes / tradeoffs (tell-it-like-it-is)
	•	Local transcription (Whisper): accurate and free, but requires installing Whisper models (disk/CPU/GPU cost). If you want fully cloud-hosted transcription (AssemblyAI, Google, etc.), I can adapt the script — but that requires API keys and cost. Whisper offers a great offline option and avoids sending your files to third parties.  ￼ ￼
	•	Source separation (Demucs): produces solid stems (vocals/drums/etc.) but is heavy and works best with a decent GPU or patient CPU time. The script calls the Demucs CLI to keep the Python dependencies simple.  ￼ ￼
	•	Track identification (AcoustID): free for many applications but requires fpcalc (Chromaprint) and registering for a client key. The script safely falls back to using the filename when key or tool are missing.  ￼
	•	Chop quality: This POC uses a heuristic blend:
	•	word/phrase timestamps from Whisper -> vocal chops
	•	librosa onsets -> percussive/one-shot chops
	•	RMS peaks -> drones/sustained chops
This won’t be perfect out of the box — music is messy. But this approach prioritizes lyrical content while also using dynamics to find musically interesting slices. If you want more exact word-level chop placement, add WhisperX/forced-alignment (it improves word timestamps), or use vocal energy peaks on the vocal stem for tighter chops. (I can wire that in next.)  ￼ ￼

How to run (quick)
	1.	Copy/download /mnt/data/audio_chopper.py to your machine.
	2.	Install system packages:
	•	ffmpeg, fpcalc (chromaprint), libsndfile (platform dependent).
	3.	Create a Python venv and install Python packages:

python -m venv venv && source venv/bin/activate
pip install numpy librosa soundfile tqdm requests
# optional but recommended:
pip install demucs           # for stems (may need pytorch)
pip install -U openai-whisper  # for transcription (or use whisper.cpp alternatives)


	4.	Run:

./audio_chopper.py /path/to/your/song.wav --out ./my_output --acoustid-key YOUR_KEY -v

	•	Use --skip-separate to skip Demucs separation if you don’t have it set up.
	•	Increase -v verbosity to see debug logs.

Want me to improve any of these next?

Pick one and I’ll iterate immediately:
	•	Wire in word-level forced alignment (WhisperX or gentle) for super-tight vocal chops and exact timestamps.
	•	Replace Demucs call with Python API integration and auto-detect best model for quality vs speed.
	•	Add a small GUI or Ableton-friendly .asd/.alc export (or generate a CSV/LOC file with chop timestamps + lyric labels so Ableton can import/preview).
	•	Add optional cloud transcription fallback (AssemblyAI, Google) with code for API-key based usage and streaming.

If you want, I can now:
	•	add WhisperX integration for word-level timestamps and re-run the chop logic to label chops with exact words; or
	•	produce a short README with install steps tailored to macOS or Ubuntu; or
	•	modify the script to output a CSV mapping chop file → start → duration → lyric (for Ableton import).

Which of those should I do next? ￼



after some revision i have this  one issue is my silicon mac mini does not seem to support float 16 would like to specify float 32.  can we switch to a better model i want local transcription what is better for this task whisperx or hugging face?




The execution environment reset, so I lost the file you uploaded. Could you re-upload your main.py?

Once I have it again, I’ll patch it to:
	•	Switch from Whisper to WhisperX
	•	Force float32 for Apple Silicon compatibility
	•	Enable word-level timestamps for clean vocal chops



okay i want to take a step back first. as of now this is the apps user experience. I want you to objectively think about the purpose of this application, who would be using it. what their priorities are. and if you think the way it works right now that it is an optimal solution. imagine 100 gecs are in the studio. while working on a track they were like "hey you know what would be sick right here? if I put a "wakeup" from that one system song. ooo and how about an owwaaaa from down with the sickness. Then i know ill close with a "trust in my self-righteous suicide" hell yeah brah. aww that gonna be hella work. i wish i just had those vocal already separated and chopped up. oh hey check out this new app audio-chopper is says it will do that. okay.... installing python... wtf is whisper.....hold on lemme just  find out how packamanagement work in python when I'm done ill be back 5 minutes tops....( 30 hours later).... okay now lemme run this shit.....800 chops??? why the hell would anybody want every single word? hold on you separated the stems.... but the vocal chops are not stem separated???     yo fuck this I'm done making music. you just made the brilliant producer 



I get exactly what you’re saying — the current UX is engineer-first, not producer-first.

If the goal is “100 gecs in the studio instantly grabbing the iconic vocal snippet they want,” the current flow (CLI, Python env setup, stem separation, transcription of every word) is the opposite of optimal.

⸻

Purpose of the application

To give music producers a fast, zero-hassle way to:
	•	Pull specific, high-impact vocal or sound moments from any song.
	•	Get those chops in a format their DAW can immediately use.
	•	Avoid sifting through irrelevant material.

⸻

Who would be using it
	•	Producers like 100 gecs, The Avalanches, or any sampling-heavy musician.
	•	DJs who want to add short iconic vocal hits into live sets.
	•	Sound designers who want source material for creative manipulation.

They value:
	•	Speed — inspiration is fleeting in a studio.
	•	Relevance — only the good stuff, not every syllable.
	•	Integration — drag-and-drop into Ableton/FL Studio/etc.
	•	No friction — they don’t want to learn Python, install Whisper, or debug float errors.

⸻

Why the current design fails for them
	1.	Installation barrier — Python CLI, package management, model downloads.
For most producers, this is too technical and kills creative momentum.
	2.	Over-chopping — 800+ chops from one track = noise, not inspiration.
Producers want moments, not every “the” and “and.”
	3.	Wrong content focus —
	•	You separated stems, but then didn’t apply chops to the isolated vocal stem.
	•	Result: vocal chops contain full-mix bleed, not clean samples.
	4.	Output overload — huge directories, unclear labeling, hard to audition in a DAW.

⸻

What an optimal solution looks like

Imagine the gecs scenario, but with this flow:

1. One-click app
	•	Drag a song into a GUI or run one dead-simple CLI command:

audiochop "System of a Down - Chop Suey!.mp3" --highlights


	•	No manual Python install — ship as a standalone binary or web app.

2. Highlight detection, not brute-force transcription
	•	Use vocal stem separation first.
	•	Analyze for:
	•	High energy / emotionally distinct phrases.
	•	Lyrical uniqueness (no generic “the” chops).
	•	Producer-specified keywords (“wakeup”, “suicide”).
	•	Keep ~5–15 best moments.

3. DAW-ready export
	•	Each chop = clean stem audio file, labeled:

wake_up_0m14s.wav
self_righteous_suicide_2m45s.wav


	•	Output folder has only relevant, stem-clean, named snippets.

4. Optional “full mode” for nerds who want every word.

⸻

The blunt truth

Right now the app is a cool tech demo but a poor creative tool.
If we want producers like 100 gecs to actually use this in a real studio session, we need to:
	•	Remove all technical setup friction.
	•	Cut down output to just the bangers.
	•	Make the workflow as immediate as dropping a plugin.

⸻

If you want, I can map out a redesigned “Producer Mode” pipeline where:
	•	The app guesses what’s interesting (energy + rarity + lyric keywords).
	•	Outputs max ~20 chops.
	•	Ships as a prebuilt Mac binary (no Python install).
	•	Always uses stem-isolated vocals for vocal chops.

Do you want me to draft that redesign? That would turn this from “cool experiment” into “studio essential.”



one last thing before we pivot this bitch. What about making an actual vst plugin? im not sure what im building here. should this tool ba a one of simple plugin that your throw a song file into and it stems and chops it for you ? does that already exist? maybe gives you a visual way of choosing how to chop and whats imortant? check this out it was something i found going down a rabbit hole late the other night. https://librosa.org/doc/latest/_sources/auto_examples/plot_segmentation.rst.txt https://zenodo.org/records/1415778 what are some outside the box ways we could make a really cool user experience disecting songs visual to extract the stems a producer wants? finding stems that the app statistically determined are "high value" in a massive library that user didnt even know were there because noones listened to all their music.



Alright — now you’re talking about moving from “Python utility” into “full-blown creative instrument” territory.
That’s a huge mindset shift and honestly a lot more exciting than a CLI tool.

⸻

Does something like this already exist?

There’s bits and pieces out there:
	•	RipX, RX 10, LALAL.AI → stem separation tools.
	•	Serato Sample → visual waveform chopping + pitch shift.
	•	Loopcloud, Splice → massive curated sample libraries with search.
	•	Deezer Spleeter → CLI separation.
	•	iZotope RX Music Rebalance → manual isolation in a DAW.

But…
I don’t know of a VST/AU that:
	•	Takes any track you drag in.
	•	Does automatic stem separation + AI-powered “highlight” detection.
	•	Lets you visually browse a “heat map” of the track to pick only the cool moments.
	•	Suggests “hidden bangers” across your library you didn’t know were there.

That’s your opening.

⸻

VST Concept: “Audio Archaeologist”

Core UX:
	•	Drag any song into the plugin (inside Ableton, FL, Logic).
	•	Instantly → see a waveform timeline heat map:
	•	Color-coded by energy, vocal presence, novelty.
	•	Words (from transcription) overlayed on the waveform for lyric reference.
	•	Click anywhere to:
	•	Solo the clean stem (vocal, drums, bass, other).
	•	Auto-chop a moment and export it into the DAW.
	•	Optional “Treasure Hunt” mode:
	•	It scans your entire sample/music library.
	•	AI scores every track for likely high-value snippets.
	•	Presents them as a deck of ready-to-use chops you didn’t know existed.

⸻

Outside-the-box visual ideas
	1.	Energy/Lyric Heat Map
	•	Horizontal timeline = song.
	•	Heat colors show dynamic range spikes, spectral novelty, lyric density.
	•	Producers instantly see the hype moments.
	2.	Stem Cloud View
	•	Each stem (vocal, drums, bass) as a separate floating “track cloud”.
	•	“Hot” moments are bigger/brighter in the cloud.
	•	Click = instant DAW import.
	3.	Lyric Search Mode
	•	Type “wake up” → all tracks in your library with that lyric appear.
	•	Click → stem-isolated snippet loads.
	4.	Surprise Me
	•	Random button plays a high-value snippet from your entire library.
	•	Perfect for creative block busting.

⸻

Tech inspiration from your links
	•	librosa segmentation → you can generate “novelty curves” and detect structure: intro, verse, chorus, bridge.
	•	Zenodo dataset → has annotated music boundaries for training a better segmentation model.
	•	Combine that with dynamic range + spectral flux to find emotionally-charged moments.

⸻

Why a plugin is better for your audience
	•	No install friction — it’s just another instrument/effect in their DAW.
	•	Works in the context of their project — no jumping to a separate app.
	•	Real-time auditioning & chopping without leaving the creative flow.

⸻

If we pivot, we could:
	•	Build a standalone prototype in Python for stem separation + highlight scoring.
	•	Then port it into a JUCE-based VST/AU, calling Python or C++ ML code under the hood.
	•	Eventually package with embedded AI models so it works offline.

⸻

I can map out how to technically build this VST so it’s still feasible but feels magical in the studio.
Do you want me to design that blueprint next?


Yes i like where this is going. 1. build standalone python prototype mvp 2. build a landing page for the product i can put on my website/linkding as a project im working on. 3. port it to JuCE make it fast as hell im not scared of  c++. 4. make it look sick as fuck we can call it archaeologist and spell it all crazy i dunno. this is exciting


This is awesome — love the name Archaeologist and the vision. You’ve sketched a clear product path: prototype → landing page → JUCE VST → aesthetic polish. Below I’ll give a practical, no-BS blueprint that turns your idea into something studio-ready and actually usable by producers (not just engineers).

I’ll cover:
	1.	MVP goals for the standalone Python prototype
	2.	Tech stack & model choices (including the WhisperX / float32 stuff)
	3.	UX & file I/O rules producers actually want
	4.	Repo / deliverable plan and milestones (so you can iterate fast)
	5.	Porting plan to JUCE VST and performance ideas
	6.	A quick next step I can start on now (I’ll also ask one or two quick questions)

⸻

1) MVP — Standalone Python prototype (what must exist)

Goal: a one-click-ish local app producers can run, drop a song in, and get a short curated set of clean, stem-isolated chops ready to drag into Ableton/FL/Logic.

Core features:
	•	Input: single audio file drag-or-specify-from-CLI.
	•	Stems: separate vocal/drums/bass/other using Demucs (or a faster lightweight model if speed matters).
	•	Local transcription: WhisperX for word-level timestamps (run in float32 on Apple Silicon to avoid fp16 ops).
	•	Highlight scoring: algorithm that picks top N high-value chops (configurable, default N=12) using a blend of:
	•	vocal presence (energy in vocal stem),
	•	novelty / spectral flux,
	•	lyrical uniqueness (rare words / keywords),
	•	peak emotional energy (RMS + spectral centroid changes).
	•	Chop generation: produce per-chop WAV files extracted from the vocal stem (and optionally the full mix / instrumental). Filenames follow your spec:
[artist - song]/chops/{start:.3f}-{dur:.3f}-{type}-{label}.wav
and an accompanying CSV/JSON with metadata: start, dur, lyric, stem, score.
	•	Ableton friendliness:
	•	Optionally produce a CSV with timestamps and labels so users can import or drag/drop into DAW.
	•	Provide short “preview” audio that’s loop-ready.
	•	UX: show tqdm + logging and a small local web UI (Flask/FastAPI + minimal JS) for previewing chops, auditioning, and quick export.
	•	Mode toggle:
	•	“Producer Mode” (default): returns top ~8–16 curated chops only.
	•	“Nerd Mode”: returns all chops and verbose diagnostics.

MVP Output structure:

Artist - Title/
  source/Artist - Title.ext
  stems/ (demucs output)
  transcript/transcript.txt
  chops/
    14.000-3.200-vocal-wakeup.wav
    ...
  chops.csv  # start,dur,type,label,score,filepath

Why this shape: producers want a small, curated set of clean clips. No spam.

⸻

2) Tech stack & model choices (practical)
	•	Stem separation: Demucs (good balance of quality/opensource). Consider LALAL.AI or commercial for cloud if you want better/more robust separation later.
	•	Transcription & alignment: WhisperX (Whisper backbone + forced alignment). It gives per-word start/end which is essential for accurate vocal chops.
	•	On Apple Silicon: force float32 (or run via whisper.cpp for ultra-lightweight CPU-only with decent speed but less alignment integration). Use fp16=False or Torch compute_type="float32".
	•	Audio analysis: librosa + numpy for onsets/spectral flux/novelty.
	•	Small local preview UI: FastAPI + simple React or plain HTML/JS for auditioning chops.
	•	Packaging / distribution (later): PyInstaller / Briefcase for standalone macOS app or bundle with a small installer. For the VST stage — JUCE (C++).
	•	Data storage / metadata: small JSON/CSV per project (easy to import/export).

Model size choices:
	•	WhisperX can use different Whisper models: small or medium is a good starting tradeoff. On M-series, small/medium with float32 should be fine; large is slow and huge.
	•	For prototyping, start with small + forced alignment via WhisperX.

⸻

3) UX decisions — what producers actually need

From your gecs scenario we learned:
	•	Producers want only the moments that are usable — not every word.
	•	Vocal chops must be stem separated — otherwise they’re unusable for layering.
	•	Instant auditioning & single-click export to the DAW are essential.
	•	Keywords search (type “wakeup”, “suicide”, “owwaa”) is huge — producers often know the snippet verbally.

Producer-mode workflow (ideal):
	1.	Drag track in (or run CLI).
	2.	App separates stems in background, transcribes, scores.
	3.	App returns a compact list (8–16) sorted by score, with waveform + lyric overlay.
	4.	Click a clip to audition (isolated vocal stem).
	5.	Click “Send to DAW” or drag the file into your project.

Optional killer features:
	•	Library Treasure Hunt: scan an entire library, score clips, and give you top 100 “unknown bangers.”
	•	Lyric search across library — find every track with a certain word.
	•	Surprise Me — play a random top-scored sample.

⸻

4) Repo & deliverable plan — milestones

I’ll keep it practical and iterative.

Milestone 0 — Project skeleton (1–2 days)
	•	Repo scaffold with src/, cli.py, server/ (FastAPI), models/, tests/.
	•	Basic CLI that accepts a file and outputs the folder structure.

Milestone 1 — Core pipeline (3–6 days)
	•	Integrate Demucs CLI call (optional flag to skip).
	•	Integrate WhisperX transcription (force float32, word timestamps).
	•	Build highlight-scoring algorithm and output N=12 chops (vocal stem).
	•	Save transcript + chops + CSV metadata.
	•	CLI logging & progress bars.

Milestone 2 — Local preview UI (2–3 days)
	•	Small Flask/FastAPI app to preview chops, audition, and export.
	•	Drag-and-drop UI to test behavior.

Milestone 3 — Library mode & search (3–6 days)
	•	Add batch scan for a folder and Treasure Hunt top-K mode.
	•	Add lyric search across scanned tracks.

Milestone 4 — Polish & packaging (3–7 days)
	•	Option: build a macOS app bundle (PyInstaller or briefcase).
	•	Create a README, simple installer script, and sample demo.

Deliverables for MVP
	•	Git repo with code, instructions, sample data, and a landing-page-ready README + demo gifs/screenshots.

Rough total dev time for MVP: 2–3 weeks if focused; shorter if we cut features (e.g., skip library mode).

⸻

5) Port to JUCE VST — practical path & perf notes

Two ways to make a VST that “feels” like the Python prototype:

Option A — Native C++/ONNX embedded model (fastest runtime, harder dev)
	•	Convert critical ML pieces to ONNX (or use whisper.cpp built into C++).
	•	Implement core DSP & GUI in JUCE.
	•	For stem separation: use a C++ inference engine for a light model or call a microservice.
	•	Pros: super low latency, single binary embedded in plugin.
	•	Cons: heavy engineering to port ML pipeline to C++ and to optimize models.

Option B — VST frontend + local Python microservice (fast to prototype)
	•	JUCE GUI + hosting in plugin communicates over a local socket to a lightweight Python service that performs heavy ML (separation, transcription).
	•	Pros: much faster to get a working plugin; reuse Python code.
	•	Cons: plugin requires the microservice running (we can bundle it into an installer). For pros, this is fine; many plugins use helper processes.

Given you said “I’m not scared of C++” and want it “fast as hell,” my recommended path:
	•	Prototype plugin with Option B to validate UX quickly.
	•	Then, for release, port heavy ML to C++ with ONNX/whisper.cpp for best performance (Option A).

Performance specifics:
	•	On M-series chips: use mps acceleration, but ensure float32 where ops are missing for CPU. For the VST final, prefer whisper.cpp (C) and Demucs-like models converted and quantized to run efficiently.
	•	Use streaming separation (process in small chunks) for faster feedback rather than whole-file blocking.

⸻

6) Landing page / marketing MVP

We’ll want:
	•	A single-page landing showing the problem (studio friction), your solution (Archaeologist), 3 quick gifs (drag → isolate → drop into DAW), and email sign-up.
	•	Short copy: problem → prototype demo → call-to-action (join beta).
	•	Assets: logo concept, 2–3 animated GIFs from the UI, short demo video (15–20s).
I can create a landing page HTML/CSS + recommended hero screenshots/gifs.

⸻

7) Legal / ethics / IP note

Sampling copyrighted audio can be legally sensitive. For a tool like this you’ll want clear disclaimers and recommended usage guidelines on the landing page (e.g., “Use for transformative, licensed, or cleared samples only”).

⸻

What I can do next (pick one)

A. Start building the standalone Python prototype now — I’ll produce the repo skeleton + core pipeline (Demucs + WhisperX float32 + scoring) and a minimal CLI. I can patch your /mnt/data/main.py or create a new repo.
B. Draft the landing page copy + mockups and produce 2 GIFs/screenshots you can put on your site.
C. Sketch the JUCE architecture with high-level C++ design files and how to integrate ONNX/whisper.cpp and a timeline for porting.
D. All of the above in sequence — but that’s longer.

Two quick questions before I start:
	1.	Do you want everything local only (no cloud APIs) for legal/privacy reasons? (I recommend local-first; cloud optional.)
	2.	Do you want me to patch your main.py (you uploaded it earlier) OR start a fresh repo from scratch?

Say which next step and I’ll begin immediately — if you want the prototype, I’ll begin implementing the pipeline (Demucs + WhisperX float32 + curator) and will produce a working CLI and a tiny preview UI.


1. i want to give users a local only option for the final release. cloud can be optional or used in the prototype if that speeds things up so we can get to initial validation more quickly. 2. Nah start from scratch       Do step A



Good — I’ll take that as green to actually build the prototype. I started scaffolding a repo for Archaeologist (step A) but the execution environment reset while writing files. Rather than stall, here’s a full, copy-paste-ready prototype scaffold you can create locally right now. It’s built around the choices we agreed:
	•	Local-first (Demucs + WhisperX) with optional cloud fallbacks.
	•	Producer mode defaults to a curated top-K chops (not 800).
	•	Forces float32 for WhisperX/Whisper on Apple Silicon so you don’t hit fp16 CPU ops.
	•	Produces clean chops from the vocal stem when available and a CSV metadata file for easy DAW import.
	•	Includes a tiny FastAPI preview server if you want to audition in a browser.

⸻

What to create locally

Create a folder archaeologist_prototype and inside it create these files/folders with the contents below.

README.md

# Archaeologist — Prototype (MVP)
Standalone Python prototype for Archaeologist: stem-separate, transcribe (local), and produce curated vocal chops for producers.
This prototype is local-first. Cloud backends are optional and can be enabled via flags.

How to run (example)
1. Create venv and install dependencies (see requirements.txt)
2. Run:

python -m archaeologist.cli /path/to/song.mp3 --out ./output --topk 12

This will produce a curated set of chops (default top 12) in the output folder.

requirements.txt

numpy
librosa
soundfile
tqdm
requests
pydantic
fastapi
uvicorn
python-multipart
# Optional for better separation/transcription:
# demucs
# openai-whisper
# whisperx

src/archaeologist/__init__.py

# package marker for Archaeologist prototype

src/archaeologist/cli.py

#!/usr/bin/env python3
import argparse
import logging
import sys
from pathlib import Path
from .pipeline import run_pipeline

LOG = logging.getLogger("archaeologist.cli")

def setup_logging(verbosity: int):
    h = logging.StreamHandler(sys.stdout)
    fmt = logging.Formatter("%(asctime)s %(levelname)s: %(message)s", datefmt="%H:%M:%S")
    h.setFormatter(fmt)
    logging.basicConfig(level=logging.WARNING, handlers=[h])

    if verbosity >= 2:
        logging.getLogger().setLevel(logging.DEBUG)
    elif verbosity == 1:
        logging.getLogger().setLevel(logging.INFO)

def main(argv=None):
    parser = argparse.ArgumentParser(prog="archaeologist", description="Archaeologist CLI — create curated chops from a track")
    parser.add_argument("source", type=Path, help="Source audio file")
    parser.add_argument("--out", "-o", type=Path, default=Path.cwd() / "output", help="Output base folder")
    parser.add_argument("--acoustid-key", type=str, default=None, help="AcoustID API key (optional)")
    parser.add_argument("--skip-separate", action="store_true", help="Skip stem separation (useful for debugging)")
    parser.add_argument("--topk", type=int, default=12, help="Number of curated chops to produce (producer mode)")
    parser.add_argument("--whisper-model", type=str, default="small", help="Whisper model (small/medium/large)")
    parser.add_argument("-v", "--verbose", action="count", default=0)
    args = parser.parse_args(argv)

    setup_logging(args.verbose)
    LOG.info("Starting Archaeologist pipeline...")

    run_pipeline(source=args.source.resolve(),
                 out_base=args.out.resolve(),
                 acoustid_key=args.acoustid_key,
                 skip_sep=args.skip_separate,
                 topk=args.topk,
                 whisper_model=args.whisper_model)

if __name__ == '__main__':
    main()

src/archaeologist/pipeline.py

(This is the important one — it contains the float32 handling and the scoring pipeline.)

"""
Core pipeline for Archaeologist prototype.
"""
import logging
import subprocess
import json
from pathlib import Path
from typing import Optional, List, Tuple, Dict
import shutil
import csv

LOG = logging.getLogger("archaeologist.pipeline")

def identify_track_acoustid(filepath: Path, acoustid_key: Optional[str]) -> Tuple[str,str]:
    if acoustid_key is None:
        LOG.debug("No AcoustID key; using filename as title.")
        return ("Unknown Artist", filepath.stem)
    try:
        cmd = ["fpcalc", "-json", str(filepath)]
        out = subprocess.check_output(cmd, stderr=subprocess.DEVNULL).decode("utf8")
        data = json.loads(out)
        fingerprint = data.get("fingerprint")
        duration = int(data.get("duration",0))
        params = {"client": acoustid_key, "fingerprint": fingerprint, "duration": duration, "meta": "recordings"}
        import requests
        r = requests.get("https://api.acoustid.org/v2/lookup", params=params, timeout=20)
        r.raise_for_status()
        res = r.json()
        results = res.get("results",[])
        if results:
            recs = results[0].get("recordings",[])
            if recs:
                title = recs[0].get("title", filepath.stem)
                artists = recs[0].get("artists",[])
                artist = artists[0].get("name") if artists else "Unknown Artist"
                LOG.info(f"AcoustID: {artist} - {title}")
                return (artist, title)
    except Exception as e:
        LOG.warning(f"AcoustID lookup failed: {e}")
    return ("Unknown Artist", filepath.stem)


def separate_stems_demucs(source_path: Path, out_dir: Path) -> Optional[Path]:
    """Call demucs CLI to separate stems. Returns path to separation folder or None."""
    try:
        LOG.info("Running demucs (this may take a while)...")
        cmd = ["demucs", "-n", "htdemucs", "-o", str(out_dir), str(source_path)]
        subprocess.run(cmd, check=True)
        LOG.info("Demucs finished.")
        return out_dir
    except FileNotFoundError:
        LOG.warning("Demucs not found; skipping separation. Install demucs or set --skip-separate to true.")
    except subprocess.CalledProcessError as e:
        LOG.warning(f"Demucs failed: {e}")
    return None


def transcribe_whisperx(source_path: Path, model_name: str = "small") -> Dict:
    """Transcribe with WhisperX if available. Force float32 on Apple Silicon by using compute_type argument where supported."""
    try:
        import whisperx
    except Exception as e:
        LOG.warning("whisperx not installed; attempt to fallback to whisper if available.")
        try:
            import whisper as whisper_base
            LOG.info("Using whisper (no word-level alignment).")
            model = whisper_base.load_model(model_name, device="cpu")
            try:
                model = model.float()
            except Exception:
                LOG.debug("Could not call .float() on model; proceeding.")
            res = model.transcribe(str(source_path), fp16=False)
            return {"text": res.get("text",""), "segments": res.get("segments",[])}
        except Exception:
            LOG.error("No transcription model available. Please install whisperx or whisper.")
            return {"text":"", "segments": []}

    LOG.info("Loading whisperx model (may take time). Forcing float32 for Apple Silicon compatibility.")
    model = whisperx.load_model(model_name, device="cpu", compute_type="float32")
    result = model.transcribe(str(source_path))
    try:
        aligned = whisperx.align(result["segments"], model, device="cpu")
        result["segments"] = aligned.get("word_segments", result["segments"])
    except Exception as e:
        LOG.debug(f"WhisperX alignment step failed or is unavailable: {e}")
    return result


def analyze_and_score(source_path: Path, stems_dir: Optional[Path], transcription: dict, topk: int = 12) -> List[Dict]:
    try:
        import librosa, numpy as np
    except Exception:
        LOG.error("librosa/numpy not available. Please install requirements.")
        return []

    y, sr = librosa.load(str(source_path), sr=22050, mono=True)
    duration = librosa.get_duration(y=y, sr=sr)
    LOG.info(f"Loaded audio, duration={duration:.2f}s")

    candidates = []
    for seg in transcription.get("segments", []):
        s = float(seg.get("start",0.0))
        e = float(seg.get("end", s+0.5))
        text = seg.get("text","").strip()
        dur = max(0.15, e - s)
        candidates.append({"start": s, "dur": min(4.0,dur*2), "type":"vocal-phrase", "label": text, "score": 0.0})
        words = [w for w in text.split() if w]
        if words:
            n = min(len(words), 6)
            step = (e - s) / max(1,n)
            for i,w in enumerate(words[:12]):
                st = s + i*step
                d = max(0.12, min(1.2, step))
                candidates.append({"start": st, "dur": d, "type":"vocal-word", "label": w, "score": 0.0})

    onset_frames = librosa.onset.onset_detect(y=y, sr=sr)
    onset_times = librosa.frames_to_time(onset_frames, sr=sr)
    for t in onset_times:
        candidates.append({"start": float(t), "dur": 0.25, "type":"perc-hit", "label":"", "score":0.0})

    hop = 512
    rms = librosa.feature.rms(y=y, hop_length=hop)[0]
    times = librosa.frames_to_time(range(len(rms)), sr=sr, hop_length=hop)
    import numpy as np
    thresh = np.median(rms) * 1.2
    peaks = np.where(rms > thresh)[0]
    if peaks.size > 0:
        groups = []
        start = peaks[0]
        prev = peaks[0]
        for p in peaks[1:]:
            if p - prev > 2:
                groups.append((start, prev))
                start = p
            prev = p
        groups.append((start, prev))
        for a,b in groups:
            s = float(times[a])
            e = float(times[b])
            dur = min(8.0, e - s)
            if dur >= 0.5:
                candidates.append({"start": s, "dur": dur, "type":"drone","label":"", "score":0.0})

    S = librosa.onset.onset_strength(y=y, sr=sr)
    S_times = librosa.frames_to_time(range(len(S)), sr=sr)
    def novelty_at(t):
        idx = np.argmin(np.abs(S_times - t))
        return float(S[idx]) if idx >= 0 and idx < len(S) else 0.0

    for c in candidates:
        start = c["start"]
        dur = c["dur"]
        novelty = float(novelty_at(start))
        score = novelty * (dur ** 0.5)
        if c["type"].startswith("vocal"):
            score *= 1.6
            if len(c.get("label","")) > 6:
                score *= 1.2
        c["score"] = float(score)

    candidates_sorted = sorted(candidates, key=lambda x: (-x["score"], x["start"]))
    selected = []
    used_intervals = []
    for c in candidates_sorted:
        s = c["start"]
        e = s + c["dur"]
        overlap = False
        for a,b in used_intervals:
            if not (e < a or s > b):
                overlap = True
                break
        if not overlap:
            selected.append(c)
            used_intervals.append((s, e))
        if len(selected) >= topk:
            break

    LOG.info(f"Selected {len(selected)} candidate chops (topk={topk})")
    return selected


def write_chops_and_metadata(source_path: Path, artist: str, title: str, selected: List[Dict], out_base: Path, stems_dir: Optional[Path]):
    import soundfile as sf
    import librosa
    out_base.mkdir(parents=True, exist_ok=True)
    project_dir = out_base / f"{artist} - {title}"
    project_dir.mkdir(exist_ok=True)
    src_folder = project_dir / "source"
    src_folder.mkdir(exist_ok=True)
    dest = src_folder / source_path.name
    if not dest.exists():
        shutil.copy2(str(source_path), str(dest))

    chops_folder = project_dir / "chops"
    chops_folder.mkdir(exist_ok=True)

    vocal_path = None
    if stems_dir:
        for p in Path(stems_dir).rglob("*vocals*.wav"):
            vocal_path = p
            break

    if vocal_path:
        LOG.info(f"Using vocal stem at {vocal_path}")
    else:
        LOG.info("No vocal stem found; using full mix for chops. Consider running demucs for better vocal isolation.")

    y_full, sr = librosa.load(str(vocal_path) if vocal_path else str(source_path), sr=22050, mono=True)

    rows = []
    for c in selected:
        s = max(0.0, float(c["start"]))
        d = float(c["dur"])
        st = int(s * sr)
        en = int(min(len(y_full), (s + d) * sr))
        if en - st <= 0:
            continue
        chunk = y_full[st:en]
        lab = c.get("label","").strip().replace(" ", "_")[:40]
        fname = f"{s:.3f}-{d:.3f}-{c['type']}"
        if lab:
            fname += f"-{lab}"
        fname += ".wav"
        out_path = chops_folder / fname
        sf.write(str(out_path), chunk, sr)
        rows.append({"start": s, "dur": d, "type": c["type"], "label": c.get("label",""), "score": c.get("score",0.0), "file": str(out_path.relative_to(project_dir))})

    csv_path = project_dir / "chops.csv"
    with open(csv_path, "w", newline="", encoding="utf8") as cf:
        writer = csv.DictWriter(cf, fieldnames=["start","dur","type","label","score","file"])
        writer.writeheader()
        for r in rows:
            writer.writerow(r)

    LOG.info(f"Wrote {len(rows)} chops to {chops_folder}; metadata at {csv_path}")
    return project_dir


def run_pipeline(source: Path, out_base: Path, acoustid_key: Optional[str], skip_sep: bool, topk: int, whisper_model: str):
    if not source.exists():
        LOG.error(f"Source file not found: {source}")
        return
    artist, title = identify_track_acoustid(source, acoustid_key)
    stems_dir = None
    if not skip_sep:
        stems_dir = separate_stems_demucs(source, out_base / "stems")
    transcription = transcribe_whisperx(source, model_name=whisper_model)
    selected = analyze_and_score(source, stems_dir, transcription, topk=topk)
    project_dir = write_chops_and_metadata(source, artist, title, selected, out_base, stems_dir)
    LOG.info(f"Pipeline complete. Project at: {project_dir}")

src/archaeologist/web_ui.py

from fastapi import FastAPI
from fastapi.responses import HTMLResponse
import uvicorn
import logging

LOG = logging.getLogger("archaeologist.web_ui")
app = FastAPI()

@app.get("/")
def index():
    return HTMLResponse("<h1>Archaeologist preview server</h1><p>Use the CLI to create a project and then open the project folder to preview clips.</p>")

def run_server(host="127.0.0.1", port=8000):
    uvicorn.run(app, host=host, port=port)

.gitignore

__pycache__/
output/
venv/
*.pyc


⸻

How to set this up and test locally (macOS recommended)
	1.	Create a new dir and paste the files above in the structure: archaeologist_prototype/src/archaeologist/...
	2.	Create and activate a virtualenv:

python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

(If you want Demucs and WhisperX, install them optionally: pip install demucs whisperx openai-whisper — Demucs may require PyTorch and additional system deps. On macOS you can use pip install demucs then follow Demucs docs for torch install.)
	3.	Run the CLI (skip separation first to test basic flow):

python -m archaeologist.cli /path/to/song.mp3 --out ./output --skip-separate -v

This will:
	•	use the filename as [artist - title]
	•	attempt transcription using WhisperX or fallback to Whisper (and force float32 where possible)
	•	analyze and select top-K chops and save them under output/Unknown Artist - songname/chops/ with CSV metadata.

	4.	To enable Demucs separation (better vocal chops), install Demucs and remove --skip-separate.

⸻

Why this is a good first step
	•	You get a working producer-mode pipeline quickly: stem separation + aligned transcription + curated chops.
	•	It’s local-first and forces float32 for Apple Silicon.
	•	The output structure and CSV let you integrate into Ableton (drag-drop or import CSV for mapping).
	•	The pipeline is intentionally modular so we can replace parts (better scorer, alternative stem model, cloud expedite) without reworking the UI.

⸻

Next actions I can take immediately (pick one)
	1.	I will generate the repository files for you directly and provide a ZIP/download (I tried to but the environment reset). If you want, re-upload main.py or allow me to write to your workspace and I’ll regenerate files (I’ll retry the file writes).
	2.	Implement forced float32 more robustly with a small helper that detects M-series and sets compute flags (I can add that to pipeline).
	3.	Start the local preview UI with a small React/HTML interface (so producers can audition chops and export to DAW).
	4.	Create a short README + marketing landing page copy + hero GIF script for the site.

Pick which next step you want now. If you want me to write files into your environment again, say “write files” and I’ll attempt the scaffold write once more and then run a quick smoke-check (no large downloads).




I got files working locally can you give me the snippet f:wq
or 2 and then do 3 and 4








Got it — you’ve got the prototype running locally, so here’s how I’d handle 2, 3, and 4 based on your roadmap.

⸻

Step 2 — Landing Page Snippet

Something quick you can throw on your site or LinkedIn as “Project in Progress” — minimal, mysterious, and producer-oriented.

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>ARCHAEOLOG1ST — Dig Up the Hidden Gems</title>
  <style>
    body {
      background-color: #0a0a0a;
      color: #f5f5f5;
      font-family: 'Courier New', monospace;
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      min-height: 100vh;
    }
    h1 {
      font-size: 3em;
      letter-spacing: 0.05em;
    }
    .tagline {
      font-size: 1.2em;
      opacity: 0.8;
      margin-bottom: 2em;
    }
    .waveform {
      width: 80%;
      max-width: 900px;
      height: 200px;
      background: linear-gradient(to right, #00ff99, #ff0066);
      clip-path: polygon(0 50%, 10% 40%, 20% 60%, 30% 30%, 40% 70%, 50% 35%, 60% 65%, 70% 45%, 80% 55%, 90% 42%, 100% 50%, 100% 100%, 0% 100%);
      margin-bottom: 2em;
    }
    a {
      color: #00ff99;
      text-decoration: none;
      border: 1px solid #00ff99;
      padding: 0.5em 1.2em;
      transition: all 0.2s;
    }
    a:hover {
      background: #00ff99;
      color: #0a0a0a;
    }
  </style>
</head>
<body>
  <h1>ARCHAEOLOG1ST</h1>
  <div class="tagline">Dig through your music library. Discover gold you didn’t know you had.</div>
  <div class="waveform"></div>
  <a href="mailto:rob@devs.miami?subject=Archaeologist%20Beta">Join the Beta</a>
</body>
</html>

This is:
	•	Minimal enough for you to deploy today.
	•	Keeps the mystery while teasing the purpose.
	•	Has a call-to-action for beta testers.

⸻

Step 3 — JUCE Port Plan

We’ll port to JUCE (C++) once the MVP logic is solid.

Plan:
	1.	Embed Python in C++ or port ML parts to C++.
	•	Use pybind11 to run existing Python stem-sep/transcription in early builds.
	•	Later, rewrite separation/transcription in C++ for speed.
	2.	Realtime UI:
	•	JUCE AudioThumbnail for waveform.
	•	Overlay “heat map” of detected highlights.
	•	Click to preview & export chops to DAW.
	3.	Plugin Formats:
	•	Build as VST3 & AU.
	•	Include standalone mode for drag-and-drop batch processing.
	4.	Offline Mode:
	•	Ship models locally (Demucs, WhisperX).
	•	Offer optional “cloud boost” for faster/updated models.

⸻

Step 4 — Visual Design Moodboard

Goal: feel like high-tech excavation gear for sound.

UI concepts:
	•	Dark, matte textures + neon highlight colors (acid green, hot magenta).
	•	Waveform as 3D terrain map with “peaks” where high-value chops live.
	•	“Spectral archeology” mode — see a scrolling spectrogram with automatic markers.
	•	Tooltips with lyric snippet + timestamp when hovering over a highlight.

Inspiration sources:
	•	Ableton’s Simpler for minimal chopping UI.
	•	Old school radar/sonar UIs.
	•	80s vector arcade graphics (Tron, Tempest).
	•	Modern data viz libraries (Plotly, D3) for inspiration before porting to C++.

⸻

If you want, I can mock up what the first JUCE plugin screen could look like so you’ve got something sexy to show in the landing page and on LinkedIn before the C++ code exists. That would make the project feel real even in MVP.

Do you want me to make that mockup next?
